# -*- coding: utf-8 -*-
"""
Created on Mon May 12 13:43:24 2025
@author: Xintang Zheng

æœŸè´§ä¸»ä¹°ä¸»å–é‡è®¡ç®—æ¨¡å— - å¹¶è¡Œç‰ˆæœ¬
ä»¿ç…§TWAPè®¡ç®—é€»è¾‘ï¼Œè®¡ç®—æ¯ä¸ªæœŸè´§å“ç§çš„ä¸»ä¹°ä¸»å–é‡‘é¢
ä½¿ç”¨concurrent.futureså®ç°å¹¶è¡Œè®¡ç®—

æ˜Ÿæ˜Ÿ: â˜… â˜† âœª âœ© ğŸŒŸ â­ âœ¨ ğŸŒ  ğŸ’« â­ï¸
å‹¾å‹¾å‰å‰: âœ“ âœ” âœ• âœ– âœ… â
æŠ¥è­¦å•¦: âš  â“˜ â„¹ â˜£
ç®­å¤´: â” âœ â™ â¤ â¥ â†© â†ª
emoji: ğŸ”” â³ â° ğŸ”’ ğŸ”“ ğŸ›‘ ğŸš« â— â“ âŒ â­• ğŸš€ ğŸ”¥ ğŸ’¡ ğŸµ ğŸ¶ ğŸ§­ ğŸ“… ğŸ¤” ğŸ§® ğŸ”¢ ğŸ“Š ğŸ“ˆ ğŸ“‰ ğŸ§  ğŸ“
"""

# %% imports
import os
import sys
from pathlib import Path
import pandas as pd
from datetime import datetime
from functools import partial
from tqdm import tqdm
import traceback
import pickle
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
import multiprocessing as mp
from itertools import product
import time

# %% add sys path
file_path = Path(__file__).resolve()
file_dir = file_path.parents[0]
project_dir = file_path.parents[0]
sys.path.append(str(project_dir))

# %%
from utils.timeutils import parse_time_string, get_a_share_intraday_time_series


# %% è®¡ç®—å•ä¸ªæœŸè´§å•æ—¥ä¸»ä¹°ä¸»å–é‡çš„å‡½æ•°
def calc_order_flow_per_fut_per_day(date, data_all, instru_id, interval='1min', keep_periods=None):
    """
    è®¡ç®—å•ä¸ªæœŸè´§å“ç§å•æ—¥çš„ä¸»ä¹°ä¸»å–é‡‘é¢
    
    å‚æ•°:
    date (str): æ—¥æœŸï¼Œæ ¼å¼ä¸º'YYYYMMDD'
    data_all (pd.DataFrame): åŒ…å«æ‰€æœ‰æœŸè´§æ•°æ®çš„DataFrame
    instru_id (str): ç›®æ ‡åˆçº¦çš„InstruIDï¼Œå¦‚'IC2401'
    interval (str): èšåˆé—´éš”ï¼Œå¦‚'1min'
    keep_periods (dict): ä¿ç•™çš„äº¤æ˜“æ—¶æ®µï¼Œå¦‚{'morning': ('09:31:00', '11:30:00'), 'afternoon': ('13:01:00', '15:00:00')}
    
    è¿”å›:
    pd.DataFrame: åŒ…å«ä¸»ä¹°ä¸»å–é‡‘é¢çš„DataFrame
    """
    if keep_periods is None:
        keep_periods = {
            'morning': ('09:31:00', '11:30:00'),
            'afternoon': ('13:01:00', '15:00:00')
        }
    
    date_in_dt = datetime.strptime(date, '%Y%m%d')
    interval_timedelta = {'seconds': parse_time_string(interval)}
    keep_ts = get_a_share_intraday_time_series(date_in_dt, interval_timedelta, 
                                               trading_periods=keep_periods)
    
    # åˆå§‹åŒ–ç»“æœDataFrame
    res = pd.DataFrame(index=keep_ts, columns=['act_buy_amount', 'act_sell_amount'])
    res['act_buy_amount'] = 0.0
    res['act_sell_amount'] = 0.0
    
    try:
        # ç­›é€‰æŒ‡å®šåˆçº¦çš„æ•°æ®
        data = data_all[data_all['InstruID'] == instru_id].copy()
        
        if data.empty:
            print(f'No data found for {instru_id} on {date}')
            return res
        
        # è®¡ç®—VWAPå’Œä¸­é—´ä»·
        data['vwap'] = data['Turnover'] / data['Volume'] / 200
        data['midprice'] = (data['BidPrice1'] + data['AskPrice1']) / 2
        data['midprice_diff'] = data['midprice'].diff()
        data['vwap_lastmpc_diff'] = (data['vwap'] - data['midprice']).shift(1)
        
        # è®¡ç®—äº¤æ˜“æ–¹å‘
        data['trade_direction'] = 0
        
        # ç¬¬ä¸€ä¼˜å…ˆçº§ï¼šmidpriceå˜åŒ–æ–¹å‘
        data.loc[data['midprice_diff'] > 0, 'trade_direction'] = 1   # midpriceä¸Šå‡ï¼Œä¸»ä¹°
        data.loc[data['midprice_diff'] < 0, 'trade_direction'] = -1  # midpriceä¸‹é™ï¼Œä¸»å–
        
        # ç¬¬äºŒä¼˜å…ˆçº§ï¼šmidpriceä¸å˜æ—¶ï¼Œæ¯”è¾ƒvwapå’Œmidprice
        midprice_unchanged = (data['midprice_diff'] == 0) | data['midprice_diff'].isna()
        data.loc[midprice_unchanged & (data['vwap'] > data['midprice']), 'trade_direction'] = 1   # vwap > midpriceï¼Œä¸»ä¹°
        data.loc[midprice_unchanged & (data['vwap'] < data['midprice']), 'trade_direction'] = -1  # vwap < midpriceï¼Œä¸»å–
        
        # ç¬¬ä¸‰ä¼˜å…ˆçº§ï¼švwap == midpriceæ—¶ï¼Œå»¶ç»­ä¸Šä¸€tickæ–¹å‘
        vwap_eq_midprice = midprice_unchanged & (data['vwap'] == data['midprice'])
        data.loc[vwap_eq_midprice, 'trade_direction'] = data['trade_direction'].shift(1).fillna(0)
        
        # è®¡ç®—æ¯ä¸ªtickçš„ä¸»ä¹°å’Œä¸»å–é‡‘é¢
        data['act_buy_amount'] = 0.0
        data['act_sell_amount'] = 0.0
        data.loc[data['trade_direction'] == 1, 'act_buy_amount'] = data.loc[data['trade_direction'] == 1, 'Turnover']
        data.loc[data['trade_direction'] == -1, 'act_sell_amount'] = data.loc[data['trade_direction'] == -1, 'Turnover']
        
        # åˆ›å»ºæ—¶é—´ç´¢å¼•
        data['DateTime'] = pd.to_datetime(data['TradDay'].astype(str) + ' ' + data['UpdateTime'].astype(str))
        data.set_index('DateTime', inplace=True)
        
        # æŒ‰æŒ‡å®šé—´éš”èšåˆ
        minute_data = data.resample(interval).agg({
            'act_buy_amount': 'sum',
            'act_sell_amount': 'sum'
        })
        
        # é‡æ–°ç´¢å¼•åˆ°ç›®æ ‡æ—¶é—´åºåˆ—
        output = minute_data.reindex(index=keep_ts)
        output = output.fillna(0.0)  # å°†NaNå¡«å……ä¸º0
        
        res.loc[:, 'act_buy_amount'] = output['act_buy_amount']
        res.loc[:, 'act_sell_amount'] = output['act_sell_amount']
        
    except Exception as e:
        traceback.print_exc()
        print(f'Error processing {instru_id} on {date}: {str(e)}')
        if date > '20250101':
            raise Exception(f'Processing error: {date}, {instru_id}')
    
    return res


# %% å¤„ç†å•ä¸ªä»»åŠ¡çš„å‡½æ•°
def process_single_task(task_params):
    """
    å¤„ç†å•ä¸ªä»»åŠ¡çš„å‡½æ•°ï¼Œç”¨äºå¹¶è¡Œè®¡ç®—
    
    å‚æ•°:
    task_params (tuple): åŒ…å«ä»»åŠ¡å‚æ•°çš„å…ƒç»„
    
    è¿”å›:
    dict: ä»»åŠ¡ç»“æœ
    """
    fut, date, curr_trade, data_base_path, save_dir, interval, keep_periods, use_cache = task_params
    
    instru_id = f'{fut}{curr_trade}'
    fut_save_dir = save_dir / fut
    cache_path = fut_save_dir / f'{date}.parquet'
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜æ–‡ä»¶
    if use_cache and cache_path.exists():
        return {
            'fut': fut,
            'date': date,
            'status': 'cached',
            'message': f'Cache exists for {instru_id} on {date}'
        }
    
    try:
        # æ„å»ºæ•°æ®æ–‡ä»¶è·¯å¾„
        data_path = f'{data_base_path}/{date}/mdl_21_1_0.csv'
        
        # è¯»å–å½“æ—¥æ•°æ®
        try:
            data_all = pd.read_csv(data_path)
        except Exception as e:
            return {
                'fut': fut,
                'date': date,
                'status': 'error',
                'message': f'æ— æ³•è¯»å–æ•°æ®æ–‡ä»¶ {data_path}: {str(e)}'
            }
        
        # è®¡ç®—å½“æ—¥ä¸»ä¹°ä¸»å–é‡
        result = calc_order_flow_per_fut_per_day(
            date=date,
            data_all=data_all,
            instru_id=instru_id,
            interval=interval,
            keep_periods=keep_periods
        )
        
        # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
        fut_save_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ç»“æœ
        result.to_parquet(cache_path)
        
        return {
            'fut': fut,
            'date': date,
            'status': 'success',
            'message': f'Successfully processed {instru_id} on {date}'
        }
        
    except Exception as e:
        error_msg = f'å¤„ç† {fut} {date} æ—¶å‡ºé”™: {str(e)}'
        if date > '20250101':
            return {
                'fut': fut,
                'date': date,
                'status': 'critical_error',
                'message': error_msg
            }
        return {
            'fut': fut,
            'date': date,
            'status': 'error',
            'message': error_msg
        }


# %% å¹¶è¡Œè®¡ç®—æ‰€æœ‰æœŸè´§å“ç§çš„ä¸»ä¹°ä¸»å–é‡
def calc_order_flow_for_all_parallel(fut_list, zhuli_dir, data_base_path, save_dir, params, 
                                    use_cache=True, max_workers=None, executor_type='process'):
    """
    å¹¶è¡Œè®¡ç®—æ‰€æœ‰æœŸè´§å“ç§çš„ä¸»ä¹°ä¸»å–é‡
    
    å‚æ•°:
    fut_list (list): æœŸè´§å“ç§åˆ—è¡¨ï¼Œå¦‚['IC', 'IF', 'IH', 'IM']
    zhuli_dir (Path): ä¸»åŠ›åˆçº¦æ•°æ®ç›®å½•
    data_base_path (str): æ•°æ®åŸºç¡€è·¯å¾„ï¼Œå¦‚'http://172.16.30.3/future-data/tonglian-data/msg_backup'
    save_dir (Path): ä¿å­˜ç›®å½•
    params (dict): å‚æ•°å­—å…¸ï¼ŒåŒ…å«intervalå’Œkeep_periods
    use_cache (bool): æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    max_workers (int): æœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨CPUæ ¸å¿ƒæ•°
    executor_type (str): æ‰§è¡Œå™¨ç±»å‹ï¼Œ'process' æˆ– 'thread'
    
    è¿”å›:
    None
    """
    interval = params.get('interval', '1min')
    keep_periods = params.get('keep_periods', {
        'morning': ('09:31:00', '11:30:00'),
        'afternoon': ('13:01:00', '15:00:00')
    })
    
    # ç¡®ä¿ä¿å­˜ç›®å½•å­˜åœ¨
    save_dir.mkdir(parents=True, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰ä»»åŠ¡
    all_tasks = []
    
    for fut in fut_list:
        print(f'å‡†å¤‡ {fut} çš„ä»»åŠ¡...')
        
        # è¯»å–ä¸»åŠ›åˆçº¦æ•°æ®
        zhuli_path = zhuli_dir / f'{fut}.parquet'
        if not zhuli_path.exists():
            print(f'ä¸»åŠ›åˆçº¦æ–‡ä»¶ä¸å­˜åœ¨: {zhuli_path}')
            continue
            
        zhuli_data = pd.read_parquet(zhuli_path)
        
        for idx in zhuli_data.index:
            date = str(zhuli_data.loc[idx, 'date'])
            curr_trade = zhuli_data.loc[idx, 'curr_trade']
            
            task_params = (
                fut, date, curr_trade, data_base_path, save_dir,
                interval, keep_periods, use_cache
            )
            all_tasks.append(task_params)
    
    print(f'æ€»å…±å‡†å¤‡äº† {len(all_tasks)} ä¸ªä»»åŠ¡')
    
    # è®¾ç½®æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
    if max_workers is None:
        max_workers = min(mp.cpu_count(), len(all_tasks))
    
    print(f'ä½¿ç”¨ {executor_type} æ‰§è¡Œå™¨ï¼Œæœ€å¤§å·¥ä½œè¿›ç¨‹æ•°: {max_workers}')
    
    # é€‰æ‹©æ‰§è¡Œå™¨ç±»å‹
    ExecutorClass = ProcessPoolExecutor if executor_type == 'process' else ThreadPoolExecutor
    
    # ç»Ÿè®¡ç»“æœ
    results = {
        'success': 0,
        'cached': 0,
        'error': 0,
        'critical_error': 0
    }
    
    start_time = time.time()
    
    # æ‰§è¡Œå¹¶è¡Œè®¡ç®—
    with ExecutorClass(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {executor.submit(process_single_task, task): task for task in all_tasks}
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm(total=len(all_tasks), desc='å¤„ç†ä»»åŠ¡') as pbar:
            for future in as_completed(future_to_task):
                task = future_to_task[future]
                
                try:
                    result = future.result()
                    status = result['status']
                    results[status] += 1
                    
                    # å¦‚æœæ˜¯ä¸¥é‡é”™è¯¯ï¼ŒæŠ›å‡ºå¼‚å¸¸
                    if status == 'critical_error':
                        print(f"ä¸¥é‡é”™è¯¯: {result['message']}")
                        raise Exception(result['message'])
                    
                    # æ›´æ–°è¿›åº¦æ¡æè¿°
                    pbar.set_postfix({
                        'æˆåŠŸ': results['success'],
                        'ç¼“å­˜': results['cached'],
                        'é”™è¯¯': results['error']
                    })
                    
                except Exception as e:
                    fut, date = task[0], task[1]
                    print(f'ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸ {fut} {date}: {str(e)}')
                    results['error'] += 1
                
                pbar.update(1)
    
    end_time = time.time()
    elapsed_time = end_time - start_time
    
    # æ‰“å°ç»Ÿè®¡ç»“æœ
    print(f'\nå¤„ç†å®Œæˆï¼æ€»è€—æ—¶: {elapsed_time:.2f} ç§’')
    print(f'æˆåŠŸå¤„ç†: {results["success"]} ä¸ªä»»åŠ¡')
    print(f'ä½¿ç”¨ç¼“å­˜: {results["cached"]} ä¸ªä»»åŠ¡')
    print(f'å¤„ç†é”™è¯¯: {results["error"]} ä¸ªä»»åŠ¡')
    print(f'ä¸¥é‡é”™è¯¯: {results["critical_error"]} ä¸ªä»»åŠ¡')
    print(f'æ€»ä»»åŠ¡æ•°: {len(all_tasks)} ä¸ª')


# %% å…¼å®¹æ€§å‡½æ•°ï¼šä¿æŒåŸæœ‰æ¥å£
def calc_order_flow_for_all(fut_list, zhuli_dir, data_base_path, save_dir, params, use_cache=True):
    """
    è®¡ç®—æ‰€æœ‰æœŸè´§å“ç§çš„ä¸»ä¹°ä¸»å–é‡ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼Œè°ƒç”¨å¹¶è¡Œç‰ˆæœ¬ï¼‰
    """
    calc_order_flow_for_all_parallel(
        fut_list=fut_list,
        zhuli_dir=zhuli_dir,
        data_base_path=data_base_path,
        save_dir=save_dir,
        params=params,
        use_cache=use_cache,
        max_workers=None,
        executor_type='process'
    )


# %% ä¸»å‡½æ•°
if __name__ == '__main__':
    # é…ç½®å‚æ•°
    fut_list = ['IC', 'IF', 'IH', 'IM']
    zhuli_dir = Path('/mnt/nfs/30.132_xt_data1/future_zhuli')
    data_base_path = 'http://172.16.30.3/future-data/tonglian-data/msg_backup'
    save_dir = Path('/mnt/Data/xintang/future_factors/trade_flow_raw')
    
    params = {
        'interval': '1min',
        'keep_periods': {
            'morning': ('09:31:00', '11:30:00'),
            'afternoon': ('13:01:00', '15:00:00')
        }
    }
    
    # æ‰§è¡Œå¹¶è¡Œè®¡ç®—
    calc_order_flow_for_all_parallel(
        fut_list=fut_list,
        zhuli_dir=zhuli_dir,
        data_base_path=data_base_path,
        save_dir=save_dir,
        params=params,
        use_cache=True,
        max_workers=None,  # è‡ªåŠ¨ä½¿ç”¨CPUæ ¸å¿ƒæ•°
        executor_type='process'  # ä½¿ç”¨è¿›ç¨‹æ± ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©'thread'
    )
    
    print('æ‰€æœ‰æœŸè´§å“ç§çš„ä¸»ä¹°ä¸»å–é‡å¹¶è¡Œè®¡ç®—å®Œæˆï¼')